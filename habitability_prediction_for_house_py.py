# -*- coding: utf-8 -*-
"""Habitability prediction for house.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-5x3SDnSvK4rrrFCtuiXdAM-3BJMNdds
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import boxcox

from google.colab import drive
drive.mount("/content/drive")

d = pd.read_csv("drive/MyDrive/train.csv")

"""# **Exploratory Data Analysis**"""

d.head()

d.shape

"""**Missing Values**

"""

d.isnull().sum()

d1 = d.copy()

def edit(x):
  x  = np.where(x.isnull(), 1, 0)
  return x

d['Dust_and_Noise'] = edit(d['Dust_and_Noise'])
d['Number_of_Windows']=edit(d['Number_of_Windows'])
d['Crime_Rate']=edit(d['Crime_Rate'])
d['Frequency_of_Powercuts']=edit(d['Frequency_of_Powercuts'])
d['Furnishing']=edit(d['Furnishing'])

import matplotlib.pyplot as plt

d.groupby('Dust_and_Noise')['Habitability_score'].median().plot.bar()

d.groupby('Number_of_Windows')['Habitability_score'].median().plot.bar()

d.groupby('Crime_Rate')['Habitability_score'].median().plot.bar()

d.groupby('Furnishing')['Habitability_score'].median().plot.bar()

d.groupby('Frequency_of_Powercuts')['Habitability_score'].median().plot.bar()

"""It was found that there were 5 variables that have missing values.
And Missing values have no impact on Dependent variable i.e. It makes no difference havin them or not having them.
"""

#d1= d1.dropna()
d1.shape

"""Numeric Features
We have two kinds of numeric features in this Dataset.

Discrete: Number_of_Windows,Number_of_Doors, Frequency_of_powercuts

Continuous: Air_Quality_Index, Traffic Density score, Property Area, Neighborhood review

Number_of_Doors and Number_of_Windows have exponential relationship with Dependent variable.
"""

d1.groupby('Frequency_of_Powercuts')['Habitability_score'].median().plot.bar()

#Slightly exponentially deacreaing graph

d1.groupby('Number_of_Windows')['Habitability_score'].median().plot.bar()

d1.groupby('Number_of_Doors')['Habitability_score'].median().plot.bar()
#Slight exponentially Increasing graph

import seaborn as sns
fig, axes = plt.subplots(nrows=3, ncols=2, figsize=(10, 12))

sns.histplot(data=d1, x="Air_Quality_Index", ax=axes[0, 0], bins =25)
sns.histplot(data=d1, x="Neighborhood_Review", ax=axes[0, 1], bins =25)
sns.histplot(data=d1, x="Traffic_Density_Score", ax=axes[1, 0], bins =25)
sns.histplot(data=d1, x="Property_Area", ax=axes[1, 1], bins =25)
sns.histplot(data=d1, x="Habitability_score", ax=axes[2, 0], bins =25)

plt.tight_layout()
plt.show()

import seaborn as sns
fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(9, 9))

sns.scatterplot(data=d1, y="Habitability_score", x="Air_Quality_Index", color ="skyblue", ax = axes[0,0])
sns.scatterplot(data=d1, y="Habitability_score", x="Neighborhood_Review", color ="skyblue", ax = axes[0,1])
sns.scatterplot(data=d1, y="Habitability_score", x="Property_Area", color ="skyblue", ax = axes[1,0])
sns.scatterplot(data=d1, y="Habitability_score", x="Traffic_Density_Score", color ="skyblue", ax = axes[1,1])


plt.tight_layout()
plt.show()

"""### Neighborhood review has positive relationship between dependent var and itself

### Outliers
"""

fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(12, 8))

sns.boxplot(y = 'Property_Area',data= d1, ax=axes[0, 0])

sns.boxplot(y = d1['Air_Quality_Index'], ax=axes[0, 1])


sns.boxplot(y = d1['Traffic_Density_Score'], ax=axes[1, 0])

sns.boxplot(y = d1['Neighborhood_Review'],ax=axes[1, 1])

plt.tight_layout()
plt.show()

"""### All of the 4 variable showcases outliers

## Categorical Variable
"""

d1.groupby('Property_Type')['Habitability_score'].median().plot.bar(color = "pink")

d1.groupby('Furnishing')['Habitability_score'].median().plot.bar(color = "pink")

d1.groupby('Power_Backup')['Habitability_score'].median().plot.bar(color = "pink")

d1.groupby('Water_Supply')['Habitability_score'].median().plot.bar(color = "pink")

d1.groupby('Crime_Rate')['Habitability_score'].median().plot.bar(color = "pink")

d1.groupby('Dust_and_Noise')['Habitability_score'].median().plot.bar(color = "pink")

"""### Furnishing has exponential decreaing relationship with dependent variable.

# **Feature Engineering**

### Handle Missing Values
"""

## So both Freq of powercuts and Number of windows have outlier so fill with median
d2=d1.copy()
d1['NoW_median'] = d1['Number_of_Windows'].fillna(d1['Number_of_Windows'].median())
d1.head()

d1['NoW_RS'] = np.where(d1['Number_of_Windows'].isnull(),d1['Number_of_Windows'].dropna().sample(random_state=85),d1['Number_of_Windows'] )
d1.head()

extreme=d1.Number_of_Windows.mean()+3*d1.Number_of_Windows.std()

d1['NoWEOD'] = d1['Number_of_Windows'].fillna(extreme)

sns.kdeplot(d1['Number_of_Windows'], label='True Values', color='red')
sns.kdeplot(d1['NoW_median'], label='Median_Impute', color='green')
sns.kdeplot(d1['NoW_RS'], label='RS_Impute', color='purple')
sns.kdeplot(d1['NoWEOD'], label='EOD_Impute', color='yellow')
plt.title('KDE Plot of True vs. Imputed Values for No of Windows')
plt.legend()
plt.show()

d1['FP_RS'] = np.where(d1['Frequency_of_Powercuts'].isnull(),d1['Frequency_of_Powercuts'].dropna().sample(random_state=85),d1['Frequency_of_Powercuts'] )
d1.head()

extreme=d1.Frequency_of_Powercuts.mean()+3*d1.Frequency_of_Powercuts.std()
d1['FP_EOD'] = d1['Frequency_of_Powercuts'].fillna(extreme)

d1['FP_median'] = d1['Frequency_of_Powercuts'].fillna(d1['Frequency_of_Powercuts'].median())
d1.head()

sns.kdeplot(d1['Frequency_of_Powercuts'], label='True Values', color='red')
sns.kdeplot(d1['FP_median'], label='Median_Impute', color='green')
sns.kdeplot(d1['FP_RS'], label='RS_Impute', color='purple')
sns.kdeplot(d1['FP_EOD'], label='EOD_Impute', color='yellow')
plt.title('KDE Plot of True vs. Imputed Values for freq of powercuts')
plt.legend()
plt.show()

### We can say that RS impute and Median impute behave almost same. EOD is bad choice.
### Selecting median impute for both freq of powercuts and No of windows.
d1 = d1.drop(columns=['FP_EOD','FP_RS','NoW_RS','NoWEOD','Number_of_Windows','Frequency_of_Powercuts'], axis =1)
d1.head()

d1['Crime_Rate'] = d1['Crime_Rate'].fillna(d1['Crime_Rate'].value_counts().index[0])
d1['Dust_and_Noise']=d1['Dust_and_Noise'].fillna(d1['Dust_and_Noise'].value_counts().index[0])
d1['Furnishing'] = d1['Furnishing'].fillna("missing")
d1.head()

d1.isnull().sum()

"""### Handling Outliers"""

### Property area, NR and air quality index are skewed
### Traffic density score IS NORMAL

#FOR NORMAL DIST HANDLING OUTLIERS

upper_bound = d1['Traffic_Density_Score'].mean() + 3*d1['Traffic_Density_Score'].std()
lower_bound = d1['Traffic_Density_Score'].mean() - 3*d1['Traffic_Density_Score'].std()
#print(upper_bound),print(lower_bound)

d1['Traffic_Density_Score'].describe()

d1[d1['Traffic_Density_Score']> upper_bound].count()

d1['Traffic_Density_Score'] = np.where(d1['Traffic_Density_Score'] < lower_bound, lower_bound, d1['Traffic_Density_Score'])
d1['Traffic_Density_Score'] = np.where(d1['Traffic_Density_Score'] > upper_bound, upper_bound, d1['Traffic_Density_Score'])

## FOR SKEWED DATA HANDLING OUTLIERS
IQR = d1['Property_Area'].quantile(0.75) - d1['Property_Area'].quantile(0.25)
lower1 = d1['Property_Area'].quantile(0.25) - 1.5*IQR
upper1 = d1['Property_Area'].quantile(0.75) + 1.5*IQR

IQR1 = d1['Neighborhood_Review'].quantile(0.75) - d1['Neighborhood_Review'].quantile(0.25)
lower2 = d1['Neighborhood_Review'].quantile(0.25) - 1.5*IQR1
upper2 = d1['Neighborhood_Review'].quantile(0.75) + 1.5*IQR1


IQR2 = d1['Air_Quality_Index'].quantile(0.75) - d1['Air_Quality_Index'].quantile(0.25)
lower3 = d1['Air_Quality_Index'].quantile(0.25) - 1.5*IQR2
upper3 = d1['Air_Quality_Index'].quantile(0.75) + 1.5*IQR2


print(lower1),print(upper1)
print(lower2),print(upper2)
print(lower3),print(upper3)

d1[d1['Neighborhood_Review'] > upper2].count()
#We don't need to do anything about the
#negative bounds and in case of NR we dont have any instances above upper so as it is

d1['Property_Area']=np.where(d1['Property_Area']> upper1, upper1, d1['Property_Area'])
d1['Air_Quality_Index']=np.where(d1['Air_Quality_Index']> upper3, upper3, d1['Air_Quality_Index'])
d1['Air_Quality_Index'] = np.where(d1['Air_Quality_Index'] < lower3, lower3, d1['Air_Quality_Index'])
d1['Neighborhood_Review']=np.where(d1['Neighborhood_Review']< lower2, lower2, d1['Neighborhood_Review'])

fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(12, 8))

sns.boxplot(y = 'Property_Area',data= d1, ax=axes[0, 0])

sns.boxplot(y = d1['Air_Quality_Index'], ax=axes[0, 1])


sns.boxplot(y = d1['Traffic_Density_Score'], ax=axes[1, 0])

sns.boxplot(y = d1['Neighborhood_Review'],ax=axes[1, 1])

plt.tight_layout()
plt.show()

"""### Data Transformation

Property_Area -----  Log Transformation
TDS as it is

Habitability score,Neighborhood Review ------- Boxcox
Air Quality Index -------- SQRT
"""

import seaborn as sns
fig, axes = plt.subplots(nrows=3, ncols=2, figsize=(10, 12))

sns.histplot(data=d1, x="Air_Quality_Index", ax=axes[0, 0], bins =25)
sns.histplot(data=d1, x="Neighborhood_Review", ax=axes[0, 1], bins =25)
sns.histplot(data=d1, x="Traffic_Density_Score", ax=axes[1, 0], bins =25)
sns.histplot(data=d1, x="Property_Area", ax=axes[1, 1], bins =25)
sns.histplot(data=d1, x="Habitability_score", ax=axes[2, 0], bins =25)

plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt
import scipy.stats as stat
import pylab

def plot_data(df,feature):
    plt.figure(figsize=(10,6))
    plt.subplot(1,2,1)
    df[feature].hist()
    plt.subplot(1,2,2)
    stat.probplot(df[feature],dist='norm',plot=pylab)
    plt.show()


#plot_data(d1,'Neighborhood_Review')
#plot_data(d1,'Traffic_Density_Score')
#plot_data(d1,'Property_Area')
#plot_data(d1,'Habitability_score

d1['AQI_Boxcox'],parameters=stat.boxcox(d1['Air_Quality_Index'])
plot_data(d1,'AQI_Boxcox')

d1['Property_Area2'] = np.log(d1['Property_Area'])
d1['Property_Area5'],m = boxcox(d1['Property_Area'])
plot_data(d1,'Property_Area2')

plot_data(d1,'Property_Area5')

d1['NR1'] = np.log(d1['Neighborhood_Review'])
d1['NR2'],m = boxcox(d1['Neighborhood_Review'])

plot_data(d1,'NR2')

plot_data(d1,'NR1')

d1.columns

d1.drop(columns =['Air_Quality_Index','Neighborhood_Review','Property_Area', 'NR2','Property_Area5'], axis = 1, inplace = True)
d1.head()

"""### Converting categorical features to numeric"""

d1 = d1[d1['Property_Type'] != "#R%$G&867"]

categorical_features=[feature for feature in d1.columns if d1[feature].dtype=='O']
categorical_features

d1['Property_Type'].value_counts()

m = d1.groupby('Property_Type')['Habitability_score'].mean().to_dict()

m

d1['Property_Type'] = d1['Property_Type'].map(m)
d1.head()

d1['Water_Supply'].value_counts()

n = d1.groupby('Water_Supply')['Habitability_score'].mean().to_dict()
n

d1['Water_Supply'] = d1['Water_Supply'].map(n)

d1['Furnishing'].value_counts()

l = d1.groupby('Furnishing')['Habitability_score'].mean().to_dict()

l

d1['Furnishing'] = d1['Furnishing'].map(l)

d1['Power_Backup'].value_counts()

d2=pd.get_dummies(d1['Power_Backup'], dtype=int)
d2.drop(columns = ['No'], axis = 1, inplace = True)
d2.head()

d1  = pd.concat([d1,d2], axis = 1)
d1.head()

d1['Crime_Rate'].value_counts()

r= d1.groupby('Crime_Rate')['Habitability_score'].mean().to_dict()
r

d1['Crime_Rate'] = d1['Crime_Rate'].map(r)
d1.head()

d1['Dust_and_Noise'].value_counts()

d3=pd.get_dummies(d1['Dust_and_Noise'], dtype=int)
d3.drop(columns = ['High'], axis = 1, inplace = True)
d3.head()

d1 = pd.concat([d1,d3], axis = 1)
d1.head()

d1.drop(columns=['Property_ID','Power_Backup','Dust_and_Noise'], axis =1, inplace= True)
d1.head()

d1.shape

"""# **Feature Selection : Correlation Analysis, Lasso**"""

correlation_matrix = d1.corr()

# Get the correlation of all columns with the target variable
correlation_with_target = correlation_matrix['Habitability_score'].drop('Habitability_score')

correlation_with_target

plt.figure(figsize=(15, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')
plt.title('Correlation Heatmap')
plt.show()

from sklearn.linear_model import Lasso
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import RobustScaler

X = d1.drop('Habitability_score', axis=1)
y = d1['Habitability_score']



X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

X_train.columns = X_train.columns.astype(str)
X_test.columns = X_test.columns.astype(str)

scaler = RobustScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

lasso = Lasso(alpha=0.3)

lasso.fit(X_train_scaled, y_train)

lasso_coef = lasso.coef_

# Create a DataFrame to display the coefficients and corresponding feature names
coef_df = pd.DataFrame({'Feature': X.columns, 'Coefficient': lasso_coef})

print("Top features selected by Lasso:")
print(coef_df.sort_values(by='Coefficient', ascending=False))



"""According to correlation and Lasso, we will select Furnishing, Water_Supply, Crime_Rate, NR2, Yes, FP_Median"""

final_df = d1.copy()

df = final_df[[ 'Furnishing','Yes','Crime_Rate','NR1','Water_Supply','Property_Area2','Traffic_Density_Score','Habitability_score']]

df.head()

"""# **Feature Scaling**"""

from sklearn.preprocessing import RobustScaler
X= df.drop('Habitability_score', axis=1)
y=df['Habitability_score']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

X_train.columns = X_train.columns.astype(str)
X_test.columns = X_test.columns.astype(str)

scaler = RobustScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

"""# **Model Training & Cross Validation**"""

from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import AdaBoostRegressor
from sklearn.linear_model import LinearRegression, Ridge,Lasso
from sklearn.neighbors import KNeighborsRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error

def evaluate_model(true, predicted):
    mae = mean_absolute_error(true, predicted)
    mse = mean_squared_error(true, predicted)
    rmse = np.sqrt(mean_squared_error(true, predicted))
    r2_square = r2_score(true, predicted)
    return mae, rmse, r2_square

## Beginning Model Training
models = {
    "Linear Regression": LinearRegression(),
    "Lasso": Lasso(),
    "Ridge": Ridge(),
    "K-Neighbors Regressor": KNeighborsRegressor(),
    "Decision Tree": DecisionTreeRegressor(),
    "Random Forest Regressor": RandomForestRegressor(),
    "Adaboost Regressor":AdaBoostRegressor()

}

for i in range(len(list(models))):
    model = list(models.values())[i]
    model.fit(X_train_scaled, y_train) # Train model

    # Make predictions
    y_train_pred = model.predict(X_train_scaled)
    y_test_pred = model.predict(X_test_scaled)

    # Evaluate Train and Test dataset
    model_train_mae , model_train_rmse, model_train_r2 = evaluate_model(y_train, y_train_pred)

    model_test_mae , model_test_rmse, model_test_r2 = evaluate_model(y_test, y_test_pred)


    print(list(models.keys())[i])

    print('Model performance for Training set')
    print("- Root Mean Squared Error: {:.4f}".format(model_train_rmse))
    print("- Mean Absolute Error: {:.4f}".format(model_train_mae))
    print("- R2 Score: {:.4f}".format(model_train_r2))

    print('----------------------------------')

    print('Model performance for Test set')
    print("- Root Mean Squared Error: {:.4f}".format(model_test_rmse))
    print("- Mean Absolute Error: {:.4f}".format(model_test_mae))
    print("- R2 Score: {:.4f}".format(model_test_r2))

    print('='*35)
    print('\n')

"""# Hyperparameter Tuning"""

from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestRegressor
param_grid = {
    'n_estimators': [100, 200, 500],
    'max_depth': [None, 10, 20, 30],

    'max_features': ['sqrt']

}

grid_search = GridSearchCV(estimator=RandomForestRegressor(), param_grid=param_grid, cv=3, n_jobs=-1, verbose=2, scoring='neg_mean_squared_error')

# Fit the model
grid_search.fit(X_train_scaled, y_train)

# Get the best parameters and score
print("Best Parameters:", grid_search.best_params_)
print("Best Score (MSE):", grid_search.best_score_)

from sklearn.metrics import mean_squared_error
best_model = grid_search.best_estimator_
y_pred = best_model.predict(X_test_scaled)

# Calculate Mean Squared Error
mse = mean_squared_error(y_test, y_pred)
rmse  = np.sqrt(mse)
print(f"Test RMSE: {rmse:.4f}")

print(f"Test Mean Squared Error: {mse:.4f}")

best_model  =  RandomForestRegressor(max_depth=20, max_features='sqrt', n_estimators=500)
best_model.fit(X_train_scaled, y_train)

y_pred = best_model.predict(X_test_scaled)

# Calculate Mean Squared Error
mse = mean_squared_error(y_test, y_pred)
rmse  = np.sqrt(mse)
print(f"Test RMSE: {rmse:.4f}")

print(f"Test Mean Squared Error: {mse:.4f}")

print(r2_score(y_test,y_pred))

"""# Final Model"""

import pickle

pickle.dump(scaler, open('/content/drive/MyDrive/scaler.pickle', 'wb'))
pickle.dump(best_model, open('/content/drive/MyDrive/best_model.pickle', 'wb'))